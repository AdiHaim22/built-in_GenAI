{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "568c0a5a",
   "metadata": {},
   "source": [
    "###  Loading the Trained Change-Type Classifier\n",
    "\n",
    "This block loads the trained **change-type classification model** from disk.\n",
    "The model is stored as a serialized dictionary (`.pkl`) that contains both the\n",
    "classifier and additional configuration metadata.\n",
    "\n",
    "- The actual scikit-learn model is retrieved from the `\"clf\"` entry.\n",
    "- The remaining entries store auxiliary information used during training\n",
    "  (e.g., DETR settings and feature names).\n",
    "- The printed class labels (`classes_`) verify the binary label mapping\n",
    "  used by the classifier.\n",
    "\n",
    "This classifier is later used to convert cached feature vectors into\n",
    "predicted labels (`y_pred`) and confidence scores (`score`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa446ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded keys: dict_keys(['clf', 'detr_model_path', 'detr_thresh', 'top_k', 'feature_names'])\n",
      "âœ… model_clf type: <class 'sklearn.linear_model._logistic.LogisticRegression'>\n",
      "âœ… classes: [0 1]\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the path relative to the Code folder\n",
    "# We check the nested path first and then the flat path\n",
    "possible_paths = [\n",
    "    r\"..\\Models\\ChangeTypeClassifier\\Change_Type_Classifier\\change_classifier_logreg.pkl\",\n",
    "    r\"..\\Models\\ChangeTypeClassifier\\change_classifier_logreg.pkl\"\n",
    "]\n",
    "\n",
    "CLF_PATH = None\n",
    "for p in possible_paths:\n",
    "    if os.path.exists(p):\n",
    "        CLF_PATH = p\n",
    "        break\n",
    "\n",
    "if CLF_PATH is None:\n",
    "    raise FileNotFoundError(\"Could not find change_classifier_logreg.pkl. Please check the Models folder.\")\n",
    "\n",
    "print(f\"Loading model from: {CLF_PATH}\")\n",
    "\n",
    "model_pack = joblib.load(CLF_PATH)\n",
    "print(\"Loaded keys:\", model_pack.keys())\n",
    "\n",
    "model_clf = model_pack[\"clf\"]\n",
    "print(\"model_clf type:\", type(model_clf))\n",
    "print(\"classes:\", getattr(model_clf, \"classes_\", None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824237c2",
   "metadata": {},
   "source": [
    "###  Building the Evaluation Table (`eval_rows.csv`)\n",
    "\n",
    "This step constructs a unified evaluation table that aligns **ground-truth labels**\n",
    "from the synthetic data generation process with **model predictions**\n",
    "produced by the trained change-type classifier.\n",
    "\n",
    "Each row in the resulting CSV corresponds to **one synthetic before/after image pair**\n",
    "and contains:\n",
    "\n",
    "- **Ground truth (`y_true`)**  \n",
    "  Derived from the metadata field `group`\n",
    "  (`substantial` / `non-substantial`).\n",
    "\n",
    "- **Model prediction (`y_pred`)** and **confidence score (`score`)**  \n",
    "  Obtained from the trained logistic-regression classifier.\n",
    "\n",
    "- **Change type**  \n",
    "  e.g., `remove_sink`, `replace_toilet2stove`.\n",
    "\n",
    "- **Image paths**  \n",
    "  Paths to the original (before) and modified (after) floorplan images.\n",
    "\n",
    "- **Auxiliary metadata**  \n",
    "  Run ID, source image index, source stem, and synthetic generation score.\n",
    "\n",
    "To ensure correct alignment between metadata entries and cached feature vectors,\n",
    "samples are matched using a unique triplet:\n",
    "\n",
    "**(change type, source image stem, synthetic index)**\n",
    "\n",
    "The resulting file, `Results/eval_rows.csv`, serves as the **single source of truth**\n",
    "for all downstream evaluation, including:\n",
    "\n",
    "- Quantitative metrics (accuracy, precision, recall, F1)\n",
    "- Confusion matrices\n",
    "- Threshold analysis\n",
    "- Performance visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81a3206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache_by_triplet: 3264\n",
      "OK records: 3264\n",
      "Matched: 3264\n",
      "Missing: 0\n",
      "âœ… Saved: c:\\Users\\adiha\\Desktop\\GenAi\\Results\\eval_rows.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_image</th>\n",
       "      <th>out_image</th>\n",
       "      <th>change_type</th>\n",
       "      <th>group</th>\n",
       "      <th>y_true</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>score</th>\n",
       "      <th>synth_score</th>\n",
       "      <th>run_id</th>\n",
       "      <th>src_index</th>\n",
       "      <th>src_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./synthetic_dataset/images_before/image_003.jpg</td>\n",
       "      <td>./synthetic_dataset/images_after/substantial/r...</td>\n",
       "      <td>remove_stove</td>\n",
       "      <td>substantial</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.614004</td>\n",
       "      <td>0.477490</td>\n",
       "      <td>20260113_143558</td>\n",
       "      <td>0</td>\n",
       "      <td>image_003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./synthetic_dataset/images_before/image_003.jpg</td>\n",
       "      <td>./synthetic_dataset/images_after/substantial/r...</td>\n",
       "      <td>remove_sink</td>\n",
       "      <td>substantial</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.632730</td>\n",
       "      <td>0.794007</td>\n",
       "      <td>20260113_143558</td>\n",
       "      <td>0</td>\n",
       "      <td>image_003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./synthetic_dataset/images_before/image_003.jpg</td>\n",
       "      <td>./synthetic_dataset/images_after/substantial/r...</td>\n",
       "      <td>replace_sink2stove</td>\n",
       "      <td>substantial</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.903136</td>\n",
       "      <td>0.794007</td>\n",
       "      <td>20260113_143558</td>\n",
       "      <td>0</td>\n",
       "      <td>image_003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./synthetic_dataset/images_before/image_003.jpg</td>\n",
       "      <td>./synthetic_dataset/images_after/non-substanti...</td>\n",
       "      <td>remove_1stdoor</td>\n",
       "      <td>non-substantial</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.336904</td>\n",
       "      <td>0.986824</td>\n",
       "      <td>20260113_143558</td>\n",
       "      <td>0</td>\n",
       "      <td>image_003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./synthetic_dataset/images_before/image_003.jpg</td>\n",
       "      <td>./synthetic_dataset/images_after/non-substanti...</td>\n",
       "      <td>remove_2sdoor</td>\n",
       "      <td>non-substantial</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.610585</td>\n",
       "      <td>0.955024</td>\n",
       "      <td>20260113_143558</td>\n",
       "      <td>0</td>\n",
       "      <td>image_003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         src_image  \\\n",
       "0  ./synthetic_dataset/images_before/image_003.jpg   \n",
       "1  ./synthetic_dataset/images_before/image_003.jpg   \n",
       "2  ./synthetic_dataset/images_before/image_003.jpg   \n",
       "3  ./synthetic_dataset/images_before/image_003.jpg   \n",
       "4  ./synthetic_dataset/images_before/image_003.jpg   \n",
       "\n",
       "                                           out_image         change_type  \\\n",
       "0  ./synthetic_dataset/images_after/substantial/r...        remove_stove   \n",
       "1  ./synthetic_dataset/images_after/substantial/r...         remove_sink   \n",
       "2  ./synthetic_dataset/images_after/substantial/r...  replace_sink2stove   \n",
       "3  ./synthetic_dataset/images_after/non-substanti...      remove_1stdoor   \n",
       "4  ./synthetic_dataset/images_after/non-substanti...       remove_2sdoor   \n",
       "\n",
       "             group  y_true  y_pred     score  synth_score           run_id  \\\n",
       "0      substantial       1       1  0.614004     0.477490  20260113_143558   \n",
       "1      substantial       1       1  0.632730     0.794007  20260113_143558   \n",
       "2      substantial       1       1  0.903136     0.794007  20260113_143558   \n",
       "3  non-substantial       0       0  0.336904     0.986824  20260113_143558   \n",
       "4  non-substantial       0       1  0.610585     0.955024  20260113_143558   \n",
       "\n",
       "   src_index   src_stem  \n",
       "0          0  image_003  \n",
       "1          0  image_003  \n",
       "2          0  image_003  \n",
       "3          0  image_003  \n",
       "4          0  image_003  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# =======================\n",
    "# 1. PATHS & SETUP\n",
    "# =======================\n",
    "# Define root relative to \"Code\" folder\n",
    "ROOT_DIR = Path(\"..\")\n",
    "\n",
    "META_PATH = ROOT_DIR / \"Data\" / \"metadata.jsonl\"\n",
    "OUT_CSV   = ROOT_DIR / \"Results\" / \"eval_rows.csv\"\n",
    "\n",
    "# Make sure Results folder exists\n",
    "OUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =======================\n",
    "# 2. LOAD CACHES (Auto-detect)\n",
    "# =======================\n",
    "# We need to find 'features_cache.pkl'. Checking both nested and flat structures.\n",
    "possible_cache_paths = [\n",
    "    ROOT_DIR / \"Models\" / \"ChangeTypeClassifier\" / \"Change_Type_Classifier\" / \"features_cache.pkl\",\n",
    "    ROOT_DIR / \"Models\" / \"ChangeTypeClassifier\" / \"features_cache.pkl\"\n",
    "]\n",
    "\n",
    "feat_cache_path = None\n",
    "for p in possible_cache_paths:\n",
    "    if p.exists():\n",
    "        feat_cache_path = p\n",
    "        break\n",
    "\n",
    "if feat_cache_path is None:\n",
    "    # If not found, initialize empty dict to avoid crash (though results will be empty)\n",
    "    print(\"Warning: 'features_cache.pkl' not found. Cache will be empty.\")\n",
    "    feat_cache = {}\n",
    "else:\n",
    "    print(f\"Loading feature cache from: {feat_cache_path}\")\n",
    "    feat_cache = joblib.load(feat_cache_path)\n",
    "\n",
    "# Ensure model_pack is loaded (from previous step, or reload if missing)\n",
    "if 'model_pack' not in locals():\n",
    "    # Try to find the model again if not in memory\n",
    "    possible_model_paths = [\n",
    "        ROOT_DIR / \"Models\" / \"ChangeTypeClassifier\" / \"Change_Type_Classifier\" / \"change_classifier_logreg.pkl\",\n",
    "        ROOT_DIR / \"Models\" / \"ChangeTypeClassifier\" / \"change_classifier_logreg.pkl\"\n",
    "    ]\n",
    "    for p in possible_model_paths:\n",
    "        if p.exists():\n",
    "            print(f\"Loading model from: {p}\")\n",
    "            model_pack = joblib.load(p)\n",
    "            break\n",
    "\n",
    "if 'model_pack' not in locals():\n",
    "    raise FileNotFoundError(\"Model not found in memory or on disk. Please run the model loading cell first.\")\n",
    "\n",
    "# Use the real sklearn model\n",
    "sk_clf = model_pack[\"clf\"]\n",
    "\n",
    "# =======================\n",
    "# 3. PROCESSING\n",
    "# =======================\n",
    "def y_true_from_group(group: str) -> int:\n",
    "    return 1 if str(group).strip().lower() == \"substantial\" else 0\n",
    "\n",
    "# Build cache mapping by (change, stem, idx) for faster lookup\n",
    "cache_by_triplet = {}\n",
    "# Regex to parse filenames like: image_001_0_synthetic.jpg\n",
    "pat = re.compile(r\"^(?P<stem>image_\\d+)_(?P<idx>\\d+)_synthetic\\.(jpg|png|jpeg)$\", re.IGNORECASE)\n",
    "\n",
    "print(\"Building cache index...\")\n",
    "for (before_p, after_p), feat in feat_cache.items():\n",
    "    # Standardize path separators\n",
    "    a = str(after_p).replace(\"\\\\\", \"/\")\n",
    "    parts = a.split(\"/\")\n",
    "    \n",
    "    if len(parts) < 2:\n",
    "        continue\n",
    "        \n",
    "    change_folder = parts[-2]\n",
    "    fname = parts[-1]\n",
    "    \n",
    "    m = pat.match(fname)\n",
    "    if not m:\n",
    "        continue\n",
    "        \n",
    "    stem = m.group(\"stem\")\n",
    "    idx  = int(m.group(\"idx\"))\n",
    "    cache_by_triplet[(change_folder, stem, idx)] = (feat, before_p, after_p)\n",
    "\n",
    "print(f\"Indexed {len(cache_by_triplet)} items in cache.\")\n",
    "\n",
    "rows = []\n",
    "missing = 0\n",
    "matched = 0\n",
    "total_ok = 0\n",
    "\n",
    "if not META_PATH.exists():\n",
    "    print(f\"Error: Metadata file not found at {META_PATH}\")\n",
    "else:\n",
    "    print(f\"Reading metadata from: {META_PATH}\")\n",
    "    with META_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            rec = json.loads(line)\n",
    "            if str(rec.get(\"status\",\"\")).lower() != \"ok\":\n",
    "                continue\n",
    "\n",
    "            total_ok += 1\n",
    "            change = rec.get(\"change\")\n",
    "            stem   = rec.get(\"src_stem\")\n",
    "            # Handle source index safely\n",
    "            try:\n",
    "                idx = int(rec.get(\"src_index\"))\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "\n",
    "            # Try to find this metadata record in our cache\n",
    "            hit = cache_by_triplet.get((change, stem, idx))\n",
    "            if hit is None:\n",
    "                missing += 1\n",
    "                continue\n",
    "\n",
    "            feat, orig_b, orig_a = hit\n",
    "            X = np.asarray(feat, dtype=np.float32).reshape(1, -1)\n",
    "\n",
    "            # Predict\n",
    "            # score = P(class=1)\n",
    "            proba = sk_clf.predict_proba(X)[0]\n",
    "            classes = list(getattr(sk_clf, \"classes_\", []))\n",
    "            \n",
    "            # Robustly get the probability for class '1' (Substantial)\n",
    "            if classes == [0, 1] or classes == [0.0, 1.0]:\n",
    "                score = float(proba[1])\n",
    "            else:\n",
    "                # Fallback: assume the last class is the positive one\n",
    "                score = float(proba[-1])\n",
    "\n",
    "            y_pred = int(score >= 0.5)\n",
    "\n",
    "            group = rec.get(\"group\")\n",
    "            y_true = y_true_from_group(group)\n",
    "\n",
    "            extra = rec.get(\"extra\", {})\n",
    "            synth_score = None\n",
    "            if isinstance(extra, dict) and \"score\" in extra:\n",
    "                try:\n",
    "                    synth_score = float(extra[\"score\"])\n",
    "                except:\n",
    "                    synth_score = None\n",
    "\n",
    "            rows.append({\n",
    "                \"src_image\": str(orig_b).replace(\"\\\\\", \"/\"),\n",
    "                \"out_image\": str(orig_a).replace(\"\\\\\", \"/\"),\n",
    "                \"change_type\": change,\n",
    "                \"group\": group,\n",
    "                \"y_true\": int(y_true),\n",
    "                \"y_pred\": int(y_pred),\n",
    "                \"score\": score,\n",
    "                \"synth_score\": synth_score,\n",
    "                \"run_id\": rec.get(\"run_id\"),\n",
    "                \"src_index\": idx,\n",
    "                \"src_stem\": stem,\n",
    "            })\n",
    "            matched += 1\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Total OK records in metadata: {total_ok}\")\n",
    "print(f\"Matched with cache: {matched}\")\n",
    "print(f\"Missing from cache: {missing}\")\n",
    "\n",
    "if rows:\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(OUT_CSV, index=False)\n",
    "    print(f\"âœ… Saved CSV to: {OUT_CSV}\")\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"Warning: No rows were matched. CSV was not created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1fc09c",
   "metadata": {},
   "source": [
    "###  Generating `Results/` and `Visuals/` from `eval_rows.csv`\n",
    "\n",
    "This block takes the evaluation table (`Results/eval_rows.csv`) and produces the final\n",
    "**numeric outputs** (saved under `Results/`) and **human-readable plots** (saved under `Visuals/`).\n",
    "\n",
    "The script performs the following steps:\n",
    "\n",
    "1. **Load evaluation data**  \n",
    "   Reads `eval_rows.csv`, which contains `y_true`, `y_pred`, model confidence (`score`), and `change_type`.\n",
    "\n",
    "2. **Overall metrics â†’ `Results/val_metrics.json`**  \n",
    "   Computes dataset-level performance:\n",
    "   - accuracy, precision, recall, F1\n",
    "   - full classification report (as JSON)\n",
    "\n",
    "3. **Confusion matrix**\n",
    "   - **Numeric matrix** saved to `Results/confusion_matrix.csv`\n",
    "   - **Heatmap visualization** saved to `Visuals/confusion_matrix.png`\n",
    "\n",
    "4. **Per-change-type breakdown â†’ `Results/per_change_type_results.csv`**  \n",
    "   Computes accuracy/precision/recall/F1 **per synthetic change type**\n",
    "   (e.g., `remove_sink`, `remove_closet`, etc.) to identify which changes are harder/easier.\n",
    "\n",
    "5. **Class distribution plot â†’ `Visuals/class_distribution.png`**  \n",
    "   Visualizes how many samples exist per `change_type` to show dataset balance.\n",
    "\n",
    "6. **Threshold sweep**\n",
    "   - Sweeps thresholds from 0.0 to 1.0 using the classifier confidence (`score`)\n",
    "   - Saves the full sweep table to `Results/threshold_sweep.csv`\n",
    "   - Plots `threshold` vs `F1` to `Visuals/threshold_vs_f1.png`\n",
    "\n",
    "7. **Error rate by confidence bins**\n",
    "   - Groups samples into confidence bins: `[0.0â€“0.4, 0.4â€“0.6, 0.6â€“0.8, 0.8â€“1.0]`\n",
    "   - Saves summary table to `Results/error_rate_by_confidence.csv`\n",
    "   - Saves bar plot to `Visuals/error_rate_by_confidence.png`\n",
    "\n",
    "After running this block, the repository contains a clean separation between:\n",
    "\n",
    "- **Results/**: machine-readable evaluation outputs (CSV/JSON)\n",
    "- **Visuals/**: figures and plots for reporting/presentations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6788c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: c:\\Users\\adiha\\Desktop\\GenAi\\Results\\eval_rows.csv rows: 3264\n",
      "âœ… Wrote: c:\\Users\\adiha\\Desktop\\GenAi\\Results\\val_metrics.json\n",
      "âœ… Wrote: c:\\Users\\adiha\\Desktop\\GenAi\\Results\\confusion_matrix.csv\n",
      "âœ… Wrote: c:\\Users\\adiha\\Desktop\\GenAi\\Visuals\\confusion_matrix.png\n",
      "âœ… Wrote: c:\\Users\\adiha\\Desktop\\GenAi\\Results\\per_change_type_results.csv\n",
      "âœ… Wrote: c:\\Users\\adiha\\Desktop\\GenAi\\Visuals\\class_distribution.png\n",
      "âœ… Wrote: c:\\Users\\adiha\\Desktop\\GenAi\\Results\\threshold_sweep.csv\n",
      "âœ… Wrote: c:\\Users\\adiha\\Desktop\\GenAi\\Visuals\\threshold_vs_f1.png\n",
      "âœ… Wrote: c:\\Users\\adiha\\Desktop\\GenAi\\Results\\error_rate_by_confidence.csv\n",
      "âœ… Wrote: c:\\Users\\adiha\\Desktop\\GenAi\\Visuals\\error_rate_by_confidence.png\n",
      "\n",
      "ðŸŽ‰ DONE. Results and Visuals created in ROOT/Results and ROOT/Visuals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adiha\\AppData\\Local\\Temp\\ipykernel_55232\\3698371715.py:154: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  err = tmp.groupby(\"bin\", dropna=False).agg(\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Paths (Script runs from ROOT/Code)\n",
    "# ---------------------------------------------------------\n",
    "# Using relative path \"..\" to step out of the Code folder\n",
    "ROOT_DIR = Path(\"..\")\n",
    "\n",
    "EVAL_CSV = ROOT_DIR / \"Results\" / \"eval_rows.csv\"\n",
    "RESULTS_DIR = ROOT_DIR / \"Results\"\n",
    "VISUALS_DIR = ROOT_DIR / \"Visuals\"\n",
    "\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "VISUALS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "if not EVAL_CSV.exists():\n",
    "    raise FileNotFoundError(f\"Evaluation CSV not found at: {EVAL_CSV}\")\n",
    "\n",
    "df = pd.read_csv(EVAL_CSV)\n",
    "print(f\"Loaded: {EVAL_CSV} | Rows: {len(df)}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1) Overall metrics -> Results/val_metrics.json\n",
    "# ---------------------------------------------------------\n",
    "y_true = df[\"y_true\"].astype(int).to_numpy()\n",
    "y_pred = df[\"y_pred\"].astype(int).to_numpy()\n",
    "\n",
    "acc = float(accuracy_score(y_true, y_pred))\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "val_metrics = {\n",
    "    \"n\": int(len(df)),\n",
    "    \"accuracy\": acc,\n",
    "    \"precision\": float(prec),\n",
    "    \"recall\": float(rec),\n",
    "    \"f1\": float(f1),\n",
    "    \"classification_report\": report,\n",
    "}\n",
    "\n",
    "(RESULTS_DIR / \"val_metrics.json\").write_text(json.dumps(val_metrics, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "print(f\"Wrote: {RESULTS_DIR / 'val_metrics.json'}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2) Confusion matrix numeric -> Results/confusion_matrix.csv\n",
    "#    + heatmap -> Visuals/confusion_matrix.png\n",
    "# ---------------------------------------------------------\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "cm_df = pd.DataFrame(cm, index=[\"true_0\",\"true_1\"], columns=[\"pred_0\",\"pred_1\"])\n",
    "cm_df.to_csv(RESULTS_DIR / \"confusion_matrix.csv\", index=True)\n",
    "print(f\"Wrote: {RESULTS_DIR / 'confusion_matrix.csv'}\")\n",
    "\n",
    "fig = plt.figure(figsize=(5,4))\n",
    "ax = plt.gca()\n",
    "im = ax.imshow(cm)\n",
    "ax.set_title(\"Confusion Matrix\")\n",
    "ax.set_xlabel(\"Predicted\")\n",
    "ax.set_ylabel(\"True\")\n",
    "ax.set_xticks([0,1]); ax.set_yticks([0,1])\n",
    "ax.set_xticklabels([\"0\",\"1\"]); ax.set_yticklabels([\"0\",\"1\"])\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax.text(j, i, str(cm[i,j]), ha=\"center\", va=\"center\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(VISUALS_DIR / \"confusion_matrix.png\", dpi=200)\n",
    "plt.close(fig)\n",
    "print(f\"Wrote: {VISUALS_DIR / 'confusion_matrix.png'}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3) Per-change-type results -> Results/per_change_type_results.csv\n",
    "# ---------------------------------------------------------\n",
    "rows = []\n",
    "for ct, g in df.groupby(\"change_type\"):\n",
    "    yt = g[\"y_true\"].astype(int).to_numpy()\n",
    "    yp = g[\"y_pred\"].astype(int).to_numpy()\n",
    "    a = float(accuracy_score(yt, yp))\n",
    "    p, r, ff, _ = precision_recall_fscore_support(yt, yp, average=\"binary\", zero_division=0)\n",
    "    rows.append({\n",
    "        \"change_type\": ct,\n",
    "        \"n\": int(len(g)),\n",
    "        \"accuracy\": a,\n",
    "        \"precision\": float(p),\n",
    "        \"recall\": float(r),\n",
    "        \"f1\": float(ff),\n",
    "    })\n",
    "\n",
    "per_ct = pd.DataFrame(rows).sort_values(\"n\", ascending=False)\n",
    "per_ct.to_csv(RESULTS_DIR / \"per_change_type_results.csv\", index=False)\n",
    "print(f\"Wrote: {RESULTS_DIR / 'per_change_type_results.csv'}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4) Class distribution plot -> Visuals/class_distribution.png\n",
    "# ---------------------------------------------------------\n",
    "counts = df[\"change_type\"].value_counts().sort_values(ascending=False)\n",
    "\n",
    "fig = plt.figure(figsize=(9,4))\n",
    "ax = plt.gca()\n",
    "ax.bar(counts.index.astype(str), counts.values)\n",
    "ax.set_title(\"Class Distribution (change_type)\")\n",
    "ax.set_xlabel(\"change_type\")\n",
    "ax.set_ylabel(\"count\")\n",
    "ax.tick_params(axis=\"x\", rotation=45)\n",
    "plt.tight_layout()\n",
    "fig.savefig(VISUALS_DIR / \"class_distribution.png\", dpi=200)\n",
    "plt.close(fig)\n",
    "print(f\"Wrote: {VISUALS_DIR / 'class_distribution.png'}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5) Threshold sweep -> Results/threshold_sweep.csv + Visuals/threshold_vs_f1.png\n",
    "# ---------------------------------------------------------\n",
    "scores = df[\"score\"].astype(float).to_numpy()\n",
    "thresholds = np.linspace(0.0, 1.0, 101)\n",
    "\n",
    "sweep_rows = []\n",
    "for thr in thresholds:\n",
    "    yp_thr = (scores >= thr).astype(int)\n",
    "    a = float(accuracy_score(y_true, yp_thr))\n",
    "    p, r, ff, _ = precision_recall_fscore_support(y_true, yp_thr, average=\"binary\", zero_division=0)\n",
    "    sweep_rows.append({\"threshold\": float(thr), \"accuracy\": a, \"precision\": float(p), \"recall\": float(r), \"f1\": float(ff)})\n",
    "\n",
    "sweep = pd.DataFrame(sweep_rows)\n",
    "sweep.to_csv(RESULTS_DIR / \"threshold_sweep.csv\", index=False)\n",
    "print(f\"Wrote: {RESULTS_DIR / 'threshold_sweep.csv'}\")\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "ax = plt.gca()\n",
    "ax.plot(sweep[\"threshold\"], sweep[\"f1\"])\n",
    "ax.set_title(\"Threshold vs F1\")\n",
    "ax.set_xlabel(\"threshold\")\n",
    "ax.set_ylabel(\"f1\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(VISUALS_DIR / \"threshold_vs_f1.png\", dpi=200)\n",
    "plt.close(fig)\n",
    "print(f\"Wrote: {VISUALS_DIR / 'threshold_vs_f1.png'}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6) Error rate by confidence bins -> Results/error_rate_by_confidence.csv + Visuals/error_rate_by_confidence.png\n",
    "# ---------------------------------------------------------\n",
    "bins = [0.0, 0.4, 0.6, 0.8, 1.0]\n",
    "labels = [f\"{bins[i]:.1f}-{bins[i+1]:.1f}\" for i in range(len(bins)-1)]\n",
    "\n",
    "tmp = df.copy()\n",
    "tmp[\"is_error\"] = (tmp[\"y_true\"].astype(int) != tmp[\"y_pred\"].astype(int)).astype(int)\n",
    "tmp[\"bin\"] = pd.cut(tmp[\"score\"].astype(float), bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "err = tmp.groupby(\"bin\", dropna=False).agg(\n",
    "    n=(\"is_error\", \"size\"),\n",
    "    error_rate=(\"is_error\", \"mean\")\n",
    ").reset_index()\n",
    "\n",
    "err.to_csv(RESULTS_DIR / \"error_rate_by_confidence.csv\", index=False)\n",
    "print(f\"Wrote: {RESULTS_DIR / 'error_rate_by_confidence.csv'}\")\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "ax = plt.gca()\n",
    "ax.bar(err[\"bin\"].astype(str), err[\"error_rate\"].fillna(0).values)\n",
    "ax.set_title(\"Error Rate by Confidence Bin\")\n",
    "ax.set_xlabel(\"score bin\")\n",
    "ax.set_ylabel(\"error_rate\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(VISUALS_DIR / \"error_rate_by_confidence.png\", dpi=200)\n",
    "plt.close(fig)\n",
    "print(f\"Wrote: {VISUALS_DIR / 'error_rate_by_confidence.png'}\")\n",
    "\n",
    "print(\"\\nDONE. Results and Visuals created in Results/ and Visuals/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
